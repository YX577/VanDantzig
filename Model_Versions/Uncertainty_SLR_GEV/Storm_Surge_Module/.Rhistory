Total_flood_frequency=array(NA,dim=c(length(X)))
total_costs=array(NA,dim=c(length(X)))
discount_factor=array(NA, dim=c(length(time)))
effective_height=array(NA, dim=c(length(X),length(time)))
p_exceed_transient=array(NA, dim=c(length(X),length(time)))
NPV_costs_flooding=array(NA, dim=c(length(X),length(time)))
subsidence=subs_rate*time
sea_level_rise=sea_level_rate*time
?subset
library(scatterplot3d)
set.seed(1234)
x <- rnorm(30, 10, 3)
y <- rnorm(30, 20, 3)
z <- rnorm(30, 50, 5)
scatterplot3d(x, y, z)
scatterplot3d(x, y, z, pch=15, color="blue", main="This is a title",
xlab="X-label", ylab="Y-label", zlab="Z-label")
scatterplot3d(x, y, z, pch=21, color="blue", bg="red", main="This is a title",
xlab="X-label", ylab="Y-label", zlab="Z-label",
type="h", angle=58)
my.2dpoints <- my.scatterplot$xyz.convert(10, 20, 50)
text(my.2dpoints$x, my.2dpoints$y, "Mean x-y-z (10,20,50)", pos=4, font=2)
my.scatterplot <- scatterplot3d(x, y, z,
pch=21, color="blue", bg="red", main="This is a title",
xlab="X-label", ylab="Y-label", zlab="Z-label",
type="h", angle=58)
my.scatterplot$points3d(10, 20, 50, col="green", pch=17, cex=2.2)
my.2dpoints <- my.scatterplot$xyz.convert(10, 20, 50)
text(my.2dpoints$x, my.2dpoints$y, "Mean x-y-z (10,20,50)", pos=4, font=2)
rm(list = ls())
graphics.off()
T = 298.15 #K       # Variables can be assigned values using "=" or "<-" (arrow sign)
S = 35 #psu
K_B = exp((-8966.9 - 2890.51 * S^0.5 - 77.942 * S
+ 1.726 * S^1.5 - 0.0993 * S^2)/T + (148.0248
+ 137.194 * S^0.5 + 1.62247 * S)
+ (-24.4344 - 25.085 * S^0.5 - 0.2474 * S) * log(T)  # 'log' is fuction for natural log in R
+ (0.053105 * S^0.5 * T))                            # log10 is function for base 10 log
rm(list = ls())
graphics.off()
T = 298.15 #K       # Variables can be assigned values using "=" or "<-" (arrow sign)
S <- 35 #psu
K_B = exp((-8966.9 - 2890.51 * S^0.5 - 77.942 * S
+ 1.726 * S^1.5 - 0.0993 * S^2)/T + (148.0248
+ 137.194 * S^0.5 + 1.62247 * S)
+ (-24.4344 - 25.085 * S^0.5 - 0.2474 * S) * log(T)  # 'log' is fuction for natural log in R
+ (0.053105 * S^0.5 * T))                            # log10 is function for base 10 log
pK_B = -log10(K_B)                        # (pH at which both species is at 50% equilibrium)
pH = seq(4, 12, by = 0.5)                 # Set vector (sequence) of pH values from 4 - 12 in increments of 0.5
pH_K = 10^-pH                             # set vector of Hydrogen activity, aH+, to achieve pH
ratio = K_B / pH_K                        # Ratio of Borate to Boric Acid (products to reactants)
Borate = (ratio / (ratio + 1)) * 100      # Percent of total solution of Borate ion
Boric_Acid =  100 - Borate                # Percent of total solution of Boric Acid (subtract from 100 to get inverse)
Proportion <- array(NA, dim = c(17,3))    # This creates a blank table with 3 columns and 17 rows, and initializes with
View(Proportion)
View(Proportion)
colnames(Proportion) <- c("pH", "% Boric Acid", "% Borate Ion")   # Sets the names of the columns in the table
View(Proportion)
Proportion[,1] <- pH              # You can isolate individual rows/columns of a matrix/table using the following notation:
Proportion[,2] <- Boric_Acid
Proportion[,3] <- Borate
View(Proportion)                  # Lets you view a variable in a new window
?lty
?col
plot(pH, Boric_Acid,                                # "plot" function begins the plot, with the x and y variable in that order
type = 'l',                                    # "type = 'l'" signifies a line plot
ylim = c(0,100),                               # "ylim" manually sets the min and max y-axis dimensions
lwd = 2,                                       # "lwd" = linewidth. Default is 1, increasing numbers get thicker
pch = 19,                                      # "pch" = point character? Google it (or type '?pch'), there are multiple different point styles to choose
cex = 0.5,                                     # "cex" = character expansion (increases/decreases point size). Default = 1
ylab = "% of total DIC",                       # "ylab" = y-axis label
xlab = "pH")                                   # "xlab" = x-axis label
abline(v = pK_B, col = "gray")                      # "abline" sets straight lines. v=vertical, h=horizontal. You set the value of the y/x intercept
abline(h = 50, col = "gray")
points(pH, Borate, type = 'o',                      # "points" tells the plot to create scatterplot with x=pH, y=Borate. Type "o" is connected points
lty = 2,                                     # "lty" = linetype. Default is 1 (regular line), 2 is dashed
col = "blue",                                # "col" = color. R has multiple colors built in that can be called by name
pch = 19,
cex = 0.5,
lwd = 2)
legend(x="topleft",                                 # "legend" creates a legend. "Topleft" indicates location on plot
inset=c(0.05,0.1),                           # inset the legend into graph
legend=c("Boric Acid", "Borate Ion"),        # sets the name for the elements in the legend
col=c("black","blue"),
lty=c("solid","dashed"),                     # sets linetypes for legend. "solid" corresponds with "Boric Acid"
lwd=c(2,2,2)
)
box(lwd = 2)                                        # "box" just puts a box outline around the plot. Here I wanted to make the outline thicker
test <- seq(1,10, 1)
plot(rnorm(100))
plot(rnorm(100), col = "red")
plot(hist(rnorm(100)))
plot(hist(rnorm(100)))
plot(rnorm(100))
point(rnorm(1000), col = "green")
points(rnorm(1000), col = "green")
?rm.NA
a <- seq(1,20, 1)
b = function(x){
a^2 + 2*a + 2
}
b = function(a){
a^2 + 2*a + 2
}
b
b(a)
plot(a, b(a))
plot(b(a), b(a)/2*b(a))
test <- rnorm(100)
test[100:120] = NA
View(test)
test2 <- sample(test, 1000)
test2 <- sample(test, 1000, replace = T)
View(test2)
?sample
?if
?if
else
help(if)
test2 <- if test =/ NA, sample(test, 1000, replace = T)
?na.omit
test2 <- na.omit(test)
View(test2)
test <- rnorm(100)
test[101:120] = NA
test2 <- na.omit(test)
"test"
print(test)
students <- data.frame(people=c("Lily", "Bo", "Jen", "Omar", "Sara", "Jack", "Ting"), team=c("Red", "Blue", "Green", "Red", "Blue", "Green", "Red"), number=c(1,2,3,5,2,7,1))
View(students)
scores <- data.frame(team=c("Black", "Blue", "Green", "Indigo", "Orange", "Red", "Violet", "White", "Yellow"), score1=c(90,96,93,88,82,84,95,89,79), score2=c(5,5,4,4,5,3,5,5,3))
View(scores)
score1 <- rep(NA, 7)
score2 <- rep(NA, 7)
students <- data.frame(students, score1, score2)
students <- data.frame(people=c("Lily", "Bo", "Jen", "Omar", "Sara", "Jack", "Ting"), team=c("Red", "Blue", "Green", "Red", "Blue", "Green", "Red"), number=c(1,2,3,5,2,7,1))
students <- data.frame(students, score1, score2)
View(students)
students$score1 <- scores$score1[match(students$team, scores$team)]
View(students)
students$score2 <- scores$score2[match(students$team, scores$team)]
students$score2 <- scores$score2[match(students$team, scores$team)]
View(students)
library(ggmap)
library(ggplot2)
library(ggmap)
install.packages("ggplot2")
library(ggmap)
library(zoo)
library(extRemes)
library(fExtremes)
load("~/Documents/Grad/SCRiM/vanDantzig/Model_Versions/Uncertainty_SLR_GEV/SLR_GEV.RData")
matplot(gev, 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev[,1], 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev[,1], 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev, 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev, 1-q, log= 'y', type = 'l',lty = 1, col = rgb(0,0,0,0.05))
dim(EV_p_exceed_transient)
hist(EV_p_exceed_transient[1,])
?rnorm
test <- rnorm(1000, 0, 1)
hist(test)
test
test
?sample
Co = 1823 mg/L
Co = 1.823 kg / m^3
[ 2 ] Lo = 2 m
Lo = 2 m
[ 3 ] v = 1.43 m/day
v = 1.65509e-05 m / s
[ 4 ] alpha = 15 cm
alpha = 0.15 m
[ 5 ] t = 0.7 day
t = 60480 s
[ 6 ] DL = alpha*v
DL = 2.48264e-06 m^2 / s
[ 7 ] Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t)))); mg/L
Co = 1823 mg/L
Co = 1823
Lo = 2
v = 1.65509e-05
alpha = 0.15
t = 0.7
DL = 2.48264e-06
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
?erfc
install.packages("pracma")
library(pracma)
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
Conc
t = 60480 s
t = 60480
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
t= 1.1 * 24 * 60 * 60
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
t = 60480
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
t= 1.1 * 24 * 60 * 60
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
Co = 1250
Ko = 0.000113426
gradient = 0.004
ne = 0.15
Lo = 25
t = 2.592e+07
v = 0.261333
log_constant = 1 / m
log_constant = 1
constant = 0.83
alpha = 1.86332
DL = 5.63597e-06
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
alpha = constant*(log10(log_constant*Lo))^2.414
DL = alpha*v
DL = 5.63597e-06
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
alpha = constant*(log10(log_constant*Lo))^2.414
alpha = 1.86332
DL = alpha*v
DL = 5.63597e-06
v = Ko * gradient/ne; m/day
log_constant = 1
alpha = constant*(log10(log_constant*Lo))^2.414
alpha = 1.86332
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
v = 0.261333
[ 15 ] log_constant = 1 /m
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
alpha = constant*(log10(log_constant*Lo))^2.414
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
Co = 1250
Ko = 0.000113426
gradient = 0.004
ne = 0.15
Lo = 25
t = 2.592e+07
v = 0.261333
log_constant = 1
constant = 0.83
alpha = constant*(log10(log_constant*Lo))^2.414
alpha = 1.86332
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
DL = 5.63597e-06
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
DL = alpha*v
DL = alpha*v
DL = 5.63597e-06
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
v = Ko * gradient/ne; m/day
alpha = 1.86332
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
Co = 1250
rm(list = ls())
Co = 1250
Ko = 0.000113426
gradient = 0.004
ne = 0.15
Lo = 25
t = 2.592e+07
v = Ko * gradient/ne; m/day
log_constant = 1
constant = 0.83
alpha = constant*(log10(log_constant*Lo))^2.414
DL = alpha*v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
Co = 1250
Ko = 0.000113426
gradient = 0.004
ne = 0.15
Lo =37
t = 2.592e+07
v = Ko * gradient/ne
v = 0.261333
v = Ko * gradient/ne
log_constant = 1
constant = 0.83
alpha = constant*(log10(log_constant*Lo))^2.414
DL = alpha*v
DL = alpha*v
alpha
v
Conc = (Co/2)*(erfc( (Lo - v*t)/(2*sqrt(DL*t)))+exp(v*Lo/DL)*erfc( (Lo + v*t)/(2*sqrt(DL*t))))
setwd("~/Documents/Grad/SCRiM/vanDantzig/Model_Versions/Uncertainty_SLR_GEV/Storm_Surge_Module")
load("MCMC_Code/SurgeMCMCResults.Rdata")
mu.burn2 <-mu[(length(xi)-10000+1):length(xi)]
xi.burn2 <- xi[(length(xi)-10000+1):length(xi)]
logsigma.burn2 <- logsigma[(length(logsigma)-10000+1):length(logsigma)]
hist(mu.burn2)
length(mu.burn2)
mu.burn3 <- subset(mu.burn2, subset = (mu.burn2 >= hpd(mu.burn2, 0.10)[1,1]) & (mu.burn2 <= hpd(mu.burn2, 0.10)[1,2]))
source("MCMC_Code/GEVMCMCSource.R")
mu.burn3 <- subset(mu.burn2, subset = (mu.burn2 >= hpd(mu.burn2, 0.10)[1,1]) & (mu.burn2 <= hpd(mu.burn2, 0.10)[1,2]))
length(mu.burn3)
mean(mu.burn2)
mean(mu.burn3)
hist(mu.burn3)
plot(mu, type = 'l')
mu.burn2 <-mu[(length(xi)-50000+1):length(xi)]
mean(mu.burn2)
mu.burn2 <-mu[(length(xi)-10000+1):length(xi)]
xi.burn2 <- xi[(length(xi)-10000+1):length(xi)]
logsigma.burn2 <- logsigma[(length(logsigma)-10000+1):length(logsigma)]
param_full <- data.frame(mu.burn2, xi.burn2, logsigma.burn2)
testdata <- param_full
onetestdat<-as.matrix(testdat) #Create into a matrix. This may be a redundant step.
testdat <- param_full
onetestdat<-as.matrix(testdat) #Create into a matrix. This may be a redundant step.
retnew<-apply(onetestdat,1,function(x) benreturn(mu=x[1],sigma=x[2],xi=x[3],t=(1.01:76.01)))
retnew<-(retnew[,5000:ncol(retnew)]) # Remove Burn-in (Step 3)
newhpd<-c(1,hpd(ret76[2,],p=0.05)[1,1:2])
newhpd<-c(1,hpd(retnew[2,],p=0.05)[1,1:2])
for(i in 2:nrow(retnew)){
newhpd<-rbind(newhpd,c(i,hpd(ret76[i,],p=0.05)[1,1:2]))
}
newhpd<-c(1,hpd(retnew[2,],p=0.1)[1,1:2])
for(i in 2:nrow(retnew)){
newhpd<-rbind(newhpd,c(i,hpd(retnew[i,],p=0.1)[1,1:2]))
}
newhpd
View(newhpd)
dim(param_full)
View(param_full)
testdat <- param_full
onetestdat<-as.matrix(testdat) #Create into a matrix. This may be a redundant step.
# For each parameter set (100k total), calculate the return level for each year. (Step 2)
retnew<-apply(onetestdat,1,function(x) benreturn(mu=x[1],xi=x[2],sigma=x[3],t=(1.01:76.01)))
retnew<-(retnew[,5000:ncol(retnew)]) # Remove Burn-in (Step 3)
# Calculate the 95% Credible interval for each return level (Step 4)
newhpd<-c(1,hpd(retnew[2,],p=0.1)[1,1:2])
for(i in 2:nrow(retnew)){
newhpd<-rbind(newhpd,c(i,hpd(retnew[i,],p=0.1)[1,1:2]))
}
View(newhpd)
library(zoo)
library(lubridate)
library(fExtremes)
source("Scripts/mycolors.R")
source("Scripts/put_fig_letter.r")
source("Scripts/plot_sf.r")
load("~/Documents/Grad/SCRiM/vanDantzig/Model_Versions/Uncertainty_SLR_GEV/Storm_Surge_Module/tide_gauge_precomputed.RData")
q = seq(0,1, length.out = 10^4)
MC_full <- sapply(1:length(xi.burn2), function(x)
{ qgev(q, xi.burn2[x], mu.burn2[x], exp(logsigma.burn2[x]))
})
upper_full.95 <- sapply(1:10^4, function (x)
{  quantile(MC_full[x,], 0.95)  })
lower_full.95 <- sapply(1:10^4, function (x)
{  quantile(MC_full[x,], 0.05)  })
# Find upper and lower (90%) limits for MCMC bounds
upper_full.90 <- sapply(1:10^4, function (x)
{  quantile(MC_full[x,], 0.9)  })
lower_full.90 <- sapply(1:10^4, function (x)
{  quantile(MC_full[x,], 0.1)  })
# Mean (expected) return levels
MC_full_mean <- apply(MC_full, 1, mean)
plot.sf(coredata(year.res.max), pch = 21, bg = "white", lwd = 1.5,
ylab = "Exceedance Probability",
xlab = "",
yaxt = 'n', yaxs = 'i',
xaxt = 'n',
ylim = c(10^-2, 10^0+0.5),
xlim = c(200, 1000))
mtext("Return Level [meters]", line = 2.5, side = 1)
axis(1, at = seq(200, 2400, by = 200), labels = seq(2, 24, by = 2), lwd = 1.5, las = 1)
polygon(x = c(rev(upper_full.90[2:10000]), (lower_full.90[2:10000])), y = c(rev(1-q[2:10000]), (1-q[2:10000])), border = NA , col = "#FF666640")
polygon(x = c(rev(year.return[,3]), (year.return[,1])), y = c(rev(1/(2:10000)), 1/(2:10000)), border = NA, col = "#0080FF40")
plot.sf(coredata(year.res.max), pch = 21, bg = "white", lwd = 1.5,
ylab = "Exceedance Probability",
xlab = "",
yaxt = 'n', yaxs = 'i',
xaxt = 'n',
ylim = c(10^-2, 10^0+0.5),
xlim = c(200, 5000))
plot.sf(coredata(year.res.max), pch = 21, bg = "white", lwd = 1.5,
ylab = "Exceedance Probability",
xlab = "",
yaxt = 'n', yaxs = 'i',
xaxt = 'n',
ylim = c(10^-2, 10^0+0.5),
xlim = c(200, 500))
mtext("Return Level [meters]", line = 2.5, side = 1)
axis(1, at = seq(200, 2400, by = 200), labels = seq(2, 24, by = 2), lwd = 1.5, las = 1)
polygon(x = c(rev(upper_full.90[2:10000]), (lower_full.90[2:10000])), y = c(rev(1-q[2:10000]), (1-q[2:10000])), border = NA , col = "#FF666640")
plot.sf(coredata(year.res.max), pch = 21, bg = "white", lwd = 1.5,
ylab = "Exceedance Probability",
xlab = "",
yaxt = 'n', yaxs = 'i',
xaxt = 'n',
ylim = c(10^-2, 10^0+0.5),
xlim = c(200, 600))
mtext("Return Level [meters]", line = 2.5, side = 1)
axis(1, at = seq(200, 2400, by = 200), labels = seq(2, 24, by = 2), lwd = 1.5, las = 1)
polygon(x = c(rev(upper_full.90[2:10000]), (lower_full.90[2:10000])), y = c(rev(1-q[2:10000]), (1-q[2:10000])), border = NA , col = "#FF666640")
polygon(x = c(rev(year.return[,3]), (year.return[,1])), y = c(rev(1/(2:10000)), 1/(2:10000)), border = NA, col = "#0080FF40")
plot.sf(coredata(year.res.max), pch = 21, bg = "white", lwd = 1.5,
ylab = "Exceedance Probability",
xlab = "",
yaxt = 'n', yaxs = 'i',
xaxt = 'n',
ylim = c(10^-2, 10^0+0.5),
xlim = c(200, 600))
mtext("Return Level [meters]", line = 2.5, side = 1)
axis(1, at = seq(200, 2400, by = 200), labels = seq(2, 24, by = 2), lwd = 1.5, las = 1)
polygon(x = c(rev(upper_full.90[2:10000]), (lower_full.90[2:10000])), y = c(rev(1-q[2:10000]), (1-q[2:10000])), border = NA , col = "#FF666640")
abline(h = 1/75)
q = seq(0,1, length.out = 10^4)
# Find closed-form solution of GEV best-fit (MLE)
fit_q_year = qgev(q, year.res.max.fit2@fit$par.ests[1], year.res.max.fit2@fit$par.ests[2], year.res.max.fit2@fit$par.ests[3])
# Find quantile function of GEV distribution for each parameter set
MC_surv <- sapply(1:length(xi.burn3), function(x)
{qgev(q, xi.burn3[x], mu.burn3[x], exp(logsigma.burn3[x]))})
# Find upper and lower limits for MCMC bounds
upper2 <- sapply(1:10^4, function (x)
{  max(MC_surv[x,])  })
lower2 <- sapply(1:10^4, function (x)
{  min(MC_surv[x,])  })
# Mean (expected) return levels
MC_surv_mean <- apply(MC_surv, 1, mean)
year.res.data <- plot.sf(coredata(year.res.max), make.plot = F)
year.res.line <- lm(log10(year.res.data)~coredata(year.res.max))
plot.sf(coredata(year.res.max), pch = 20,
ylab = "Probability",
xlab = "Return Level [meters]",
yaxt = 'n', yaxs = 'i',
xaxt = 'n',
ylim = c(10^-2, 10^0+0.5),
xlim = c(200, 600))
axis(1, at = seq(200, 1200, by = 200), labels = seq(2, 12, by = 2), lwd = 1.5, las = 1)
polygon(x = c(rev(upper2[2:10000]), (lower2[2:10000])), y = c(rev(1-q[2:10000]), (1-q[2:10000])), border = NA , col = rgb(10/255, 10/255, 10/255, 0.1))
View(MC_surv)
hist(MC_surv[75,]
)
testdat <- param_full
onetestdat<-as.matrix(testdat) #Create into a matrix. This may be a redundant step.
# For each parameter set (100k total), calculate the return level for each year. (Step 2)
retnew<-apply(onetestdat,1,function(x) benreturn(mu=x[1],xi=x[2],sigma=exp(x[3]),t=(1.01:76.01)))
retnew<-(retnew[,5000:ncol(retnew)]) # Remove Burn-in (Step 3)
# Calculate the 95% Credible interval for each return level (Step 4)
newhpd<-c(1,hpd(retnew[2,],p=0.1)[1,1:2])
for(i in 2:nrow(retnew)){
newhpd<-rbind(newhpd,c(i,hpd(retnew[i,],p=0.1)[1,1:2]))
}
View(newhpd)
abline(h = 75)
abline(h = 1/75)
max(MC_surv[75,])
dim(MC_surv)
max(MC_surv[10000,])
max(MC_surv[9999,])
upper2
upper2[75]
polygon(x = c(rev(upper2[2:10000]), (lower2[2:10000])), y = c(rev(1-q[2:10000]), (1-q[2:10000])), border = NA , col = rgb(10/255, 10/255, 10/255, 0.1))
mean(exp(logsigma.burn2))
e^3
exp(3)
exp(3.8)
year.return <- return.level(year.res.max.fit, return.period = c(2:10000), alpha = 0.1, do.ci = TRUE)
library(extRemes)
library(fExtremes)
library(ismev)
year.res.max.fit <- fevd(coredata(year.res.max))   # extRemes package
year.res.max.fit2 <- gevFit(coredata(year.res.max))   # fExtremes package
year.res.max.fit3 <- gev.fit(coredata(year.res.max), show = FALSE)   # ismev package
year.return <- return.level(year.res.max.fit, return.period = c(2:10000), alpha = 0.1, do.ci = TRUE)
View(year.return)
year.return
year.return[1:"100-year return level"]
class(year.return)
class(year.return["100-year return level"])
year.return["100-year return level"]
year.return[1:100]
year.return[1:75]
year.return <- return.level(year.res.max.fit, return.period = c(1:75), alpha = 0.1, do.ci = TRUE)
year.return <- return.level(year.res.max.fit, return.period = c(2:76), alpha = 0.1, do.ci = TRUE)
year.return
retnew<-apply(onetestdat,1,function(x) benreturn(mu=x[1],xi=x[2],sigma=exp(x[3]),t=(1.01:10000.01)))
retnew<-(retnew[,5000:ncol(retnew)]) # Remove Burn-in (Step 3)
newhpd<-c(1,hpd(retnew[2,],p=0.1)[1,1:2])
for(i in 2:nrow(retnew)){
newhpd<-rbind(newhpd,c(i,hpd(retnew[i,],p=0.1)[1,1:2]))
}
View(newhpd)
year.return <- return.level(year.res.max.fit, return.period = c(2:10000), alpha = 0.1, do.ci = TRUE)
year.return
year.return[950:10000]
tail(year.return)
tail(year.return$estimate)
summary(year.return)
min(MC_surv[,10000])
min(MC_surv[10000,])
max(MC_surv[10000,])
max(MC_surv[9999,])
gev.diag(year.res.max.fit3)
gev.diag
gev.rl
returnPlot(year.res.max.fit2)
returnPlot(year.res.max.fit)
returnPlot(year.res.max.fit3)
gev.diag(year.res.max.fit3)$rl
plot(year.res.max.fit)
plot(year.res.max.fit2)
dev.off()
gev.diag(year.res.max.fit3)
